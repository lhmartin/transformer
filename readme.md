# Attention Is All You Need - PyTorch Transformer Implementation

In this repo I aspire to re-implement using PyTorch the original [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper, laid out a model for translating between german and english. It introduced a new form of the attention mechanism and the creation of the Transformer model. Which kicked of a whole new era of NLP models and beyond.

## What is a Transformer?

A transformer is a model architecture 

<img src="imgs/Figure 1 - The Transformer.png" alt="The Transformer" width="500"/>

## The Attention Mechanism
<img src="imgs/Figure 2 - Scaled Dot Attention and Multi Head.png" alt="Scaled Dot Attention and Multi Head" width="700"/>

## Training Task and Data

## Components

### Sequential Embeddings

![alt text](<imgs/pos encoding.png>)

### Custom Learning Rate

