# Attention Is All You Need - PyTorch Transformer Implementation

In this repo I aspire to re-implement using PyTorch.

The original [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper, laid out a model for translating between german and english. It introduced a new form of the attention mechanism and the creation of the Transformer model. Which kicked of a whole new era of NLP models and beyond.

## What is a Transformer?

![The Transformer](<imgs/Figure 1 - The Transformer.png>)

## The Attention Mechanism
![Scaled Dot Attention and Multi Head](<imgs/Figure 2 - Scaled Dot Attention and Multi Head.png>)

## Training Task and Data

## Components

### Sequential Embeddings

![alt text](<imgs/pos encoding.png>)

### Custom Learning Rate

