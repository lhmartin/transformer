from modules.multi_head_attention import MultiHeadAttention
import torch.nn as nn


class TransfomerBlock(nn.Module):
    
    def __init__(self):
        super().__init__()
        

    def forward(self):
        return

class Transformer(nn.Module):
    
    def __init__(self,
                 num_blocks : int = 10):
        return
    
    def forward(self):
        return